{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c55773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count Program using Apache Spark\n",
    "\n",
    "# This notebook demonstrates how to perform a simple word count using Apache Spark. \n",
    "# The program reads a text file, \n",
    "# splits the lines into words, \n",
    "# counts the occurrences of each word, \n",
    "# and displays the results.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PysparkExample\").getOrCreate()\n",
    "\n",
    "# Example: Create a DataFrame\n",
    "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "df.show()\n",
    "\n",
    "# You can still access the underlying SparkContext if needed:\n",
    "sc = spark.sparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3542c8",
   "metadata": {},
   "source": [
    "include '%pip install pyspark' if pyspark is not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10216137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/20 11:31:46 WARN Utils: Your hostname, Nikhils-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.37 instead (on interface en0)\n",
      "25/06/20 11:31:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/20 11:31:46 WARN Utils: Your hostname, Nikhils-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.37 instead (on interface en0)\n",
      "25/06/20 11:31:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/20 11:31:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/20 11:31:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 4\n",
      "for: 2\n",
      "word: 1\n",
      "Pyspark: 1\n",
      "Programs.ipynb: 1\n",
      "just: 1\n",
      "Sample: 2\n",
      "pyspark: 1\n",
      "count: 1\n",
      "program: 1\n",
      "Refer: 1\n",
      "is: 1\n",
      "a: 1\n",
      "reference: 1\n",
      "+----------+-------+-----+\n",
      "|student_id|subject|marks|\n",
      "+----------+-------+-----+\n",
      "|       101|   Math|   90|\n",
      "|       102|Science|   85|\n",
      "|       103|English|   88|\n",
      "+----------+-------+-----+\n",
      "\n",
      "+----------+-------+-----+\n",
      "|student_id|subject|marks|\n",
      "+----------+-------+-----+\n",
      "|       101|   Math|   90|\n",
      "|       102|Science|   85|\n",
      "|       103|English|   88|\n",
      "+----------+-------+-----+\n",
      "\n",
      "+----------+-------+-----+\n",
      "|student_id|subject|marks|\n",
      "+----------+-------+-----+\n",
      "|       101|   Math|   90|\n",
      "|       103|English|   88|\n",
      "+----------+-------+-----+\n",
      "\n",
      "+----------+-------+-----+\n",
      "|student_id|subject|marks|\n",
      "+----------+-------+-----+\n",
      "|       101|   Math|   90|\n",
      "|       103|English|   88|\n",
      "+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create the resources directory and sample file if they don't exist\n",
    "os.makedirs(\"resources\", exist_ok=True)\n",
    "sample_file_path = \"resources/samplefile_pyspark_wordcount.txt\"\n",
    "if not os.path.exists(sample_file_path):\n",
    "    with open(sample_file_path, \"w\") as f:\n",
    "        f.write(\"hello world\\nhello spark\\nhello world\\n\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "# SparkSession is the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "# It allows you to create DataFrames, register DataFrames as tables, and execute SQL over tables.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the text file\n",
    "# Check if the file exists and is accessible before reading\n",
    "sample_file_path = \"resources/samplefile_pyspark_wordcount.txt\"\n",
    "\n",
    "# Using SparkContext to read the text file\n",
    "sc = spark.sparkContext\n",
    "text_file = sc.textFile(sample_file_path)\n",
    "\n",
    "# Split each line into words\n",
    "words = text_file.flatMap(lambda line: line.split())\n",
    "\n",
    "# Map each word to a (word, 1) pair\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key to count occurrences\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect and display the results\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Example: Create a DataFrame from a list of tuples\n",
    "sample_data = [(101, 'Math', 90), (102, 'Science', 85), (103, 'English', 88)]\n",
    "df = spark.createDataFrame(sample_data, [\"student_id\", \"subject\", \"marks\"])\n",
    "df.show()\n",
    "\n",
    "# Example: Perform a simple transformation\n",
    "# Filter students with marks greater than 85\n",
    "filtered_df = df.filter(df.marks > 85)\n",
    "filtered_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
