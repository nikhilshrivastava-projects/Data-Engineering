{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1140eafc",
   "metadata": {},
   "source": [
    "# Pyspark Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55de876",
   "metadata": {},
   "source": [
    "Refer this page for Pyspark and Delta table optimization techniques: https://www.databricks.com/discover/pages/optimize-data-workloads-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4fc6e4a3f344d0",
   "metadata": {},
   "source": [
    "PySpark Optimization Techniques for Data Engineers\n",
    "Optimizing PySpark performance is essential for efficiently processing large-scale data. Here are some key optimization techniques to enhance the performance of your PySpark applications:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3427bb2ec149d16",
   "metadata": {},
   "source": [
    "Use Broadcast Variables\n",
    "When joining smaller DataFrames with larger ones, consider using broadcast variables. This technique helps in distributing smaller DataFrames to all worker nodes, reducing data shuffling during the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f001a95e29f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "small_df = spark.createDataFrame([...])\n",
    "large_df = spark.createDataFrame([...])\n",
    "\n",
    "result_df = large_df.join(broadcast(small_df), \"common_column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac173afa88e265",
   "metadata": {},
   "source": [
    "Partitioning\n",
    "Ensure that your DataFrames are properly partitioned to optimize data distribution across worker nodes. Choose appropriate partitioning columns to minimize data shuffling during transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5842199364707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(\"column_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c38320f9423dc",
   "metadata": {},
   "source": [
    "Persist Intermediate Results\n",
    "If you have multiple operations on the same DataFrame, consider persisting the intermediate results in memory or disk. This prevents recomputation and improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef66b9e7a5013a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ff0ac5d5420db",
   "metadata": {},
   "source": [
    "Adjust Memory Configurations\n",
    "Tune the memory configurations for your PySpark application based on the available resources. This includes configuring executor memory, driver memory, and other related parameters in the SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeddba9cc33f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().set(\"spark.executor.memory\", \"4g\").set(\"spark.driver.memory\", \"2g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676065db53d49652",
   "metadata": {},
   "source": [
    "Use DataFrames API Instead of RDDs\n",
    "The DataFrame API in PySpark is optimized and performs better than the RDD API. Whenever possible, prefer using DataFrames for transformations and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200bef85e37c60a",
   "metadata": {},
   "source": [
    "Avoid Using UDFs (User-Defined Functions) When Not Necessary\n",
    "User-Defined Functions in PySpark can be less performant than built-in functions. If there’s an equivalent built-in function, use it instead of a UDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bd83b1f1e3de9",
   "metadata": {},
   "source": [
    "Use Spark SQL Caching\n",
    "Leverage Spark SQL’s caching mechanism to cache tables or DataFrames in memory, especially for frequently accessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526beb360e061af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CACHE TABLE your_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5afcf820b5b132",
   "metadata": {},
   "source": [
    "Use Catalyst Optimizer and Tungsten Execution Engine\n",
    "PySpark utilizes the Catalyst optimizer and Tungsten execution engine to optimize query plans. Keep your PySpark version updated to benefit from the latest optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6dc6cf71ff8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Increase Parallelism\n",
    "Adjust the level of parallelism by configuring the number of partitions in transformations like repartition or coalesce. This can enhance the parallel execution of tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
